# pylint: disable=too-many-lines
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
import json
import sys
from typing import Any, Callable, Dict, IO, List, Optional, TypeVar, Union, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.pipeline.transport import AsyncHttpResponse
from azure.core.rest import HttpRequest
from azure.core.tracing.decorator_async import distributed_trace_async
from azure.core.utils import case_insensitive_dict

from ... import models as _models
from ..._model_base import AzureJSONEncoder, _deserialize
from ..._operations._operations import (
    build_authoring_cancel_fine_tune_request,
    build_authoring_create_fine_tune_request,
    build_authoring_delete_deployment_request,
    build_authoring_delete_file_request,
    build_authoring_delete_fine_tune_request,
    build_authoring_get_deployment_request,
    build_authoring_get_file_content_request,
    build_authoring_get_file_request,
    build_authoring_get_fine_tune_request,
    build_authoring_get_model_request,
    build_authoring_import_file_request,
    build_authoring_list_deployments_request,
    build_authoring_list_files_request,
    build_authoring_list_fine_tune_events_request,
    build_authoring_list_fine_tunes_request,
    build_authoring_list_models_request,
    build_authoring_update_deployment_request,
    build_authoring_upload_file_request,
    build_inference_completions_request,
    build_inference_embeddings_request,
)
from .._vendor import AuthoringClientMixinABC, InferenceClientMixinABC

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]


class AuthoringClientOperationsMixin(AuthoringClientMixinABC):
    @distributed_trace_async
    async def list_deployments(self, **kwargs: Any) -> _models.DeploymentList:
        """Gets the list of deployments owned by the Azure OpenAI resource.

        Gets the list of deployments owned by the Azure OpenAI resource.

        :return: DeploymentList. The DeploymentList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.DeploymentList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.DeploymentList] = kwargs.pop("cls", None)

        request = build_authoring_list_deployments_request(
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.DeploymentList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_deployment(self, deployment_id: str, **kwargs: Any) -> _models.Deployment:
        """Gets details for a single deployment specified by the given deployment_id.

        Gets details for a single deployment specified by the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.Deployment] = kwargs.pop("cls", None)

        request = build_authoring_get_deployment_request(
            deployment_id=deployment_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.Deployment, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def update_deployment(
        self,
        deployment_id: str,
        body: _models.Deployment,
        *,
        content_type: str = "application/merge-patch+json",
        **kwargs: Any
    ) -> _models.Deployment:
        """Updates the mutable details of the deployment with the given deployment_id.

        Updates the mutable details of the deployment with the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :param body: Required.
        :type body: ~azure.openai.python.models.Deployment
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/merge-patch+json".
        :paramtype content_type: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def update_deployment(
        self, deployment_id: str, body: JSON, *, content_type: str = "application/merge-patch+json", **kwargs: Any
    ) -> _models.Deployment:
        """Updates the mutable details of the deployment with the given deployment_id.

        Updates the mutable details of the deployment with the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/merge-patch+json".
        :paramtype content_type: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def update_deployment(
        self, deployment_id: str, body: IO, *, content_type: str = "application/merge-patch+json", **kwargs: Any
    ) -> _models.Deployment:
        """Updates the mutable details of the deployment with the given deployment_id.

        Updates the mutable details of the deployment with the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :param body: Required.
        :type body: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/merge-patch+json".
        :paramtype content_type: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def update_deployment(
        self, deployment_id: str, body: Union[_models.Deployment, JSON, IO], **kwargs: Any
    ) -> _models.Deployment:
        """Updates the mutable details of the deployment with the given deployment_id.

        Updates the mutable details of the deployment with the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :param body: Is one of the following types: model, JSON, IO Required.
        :type body: ~azure.openai.python.models.Deployment or JSON or IO
        :keyword content_type: This request has a JSON Merge Patch body. Default value is None.
        :paramtype content_type: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Deployment] = kwargs.pop("cls", None)

        content_type = content_type or "application/merge-patch+json"
        _content = None
        if isinstance(body, (IO, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=AzureJSONEncoder)  # type: ignore

        request = build_authoring_update_deployment_request(
            deployment_id=deployment_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if response.status_code == 200:
            deserialized = _deserialize(_models.Deployment, response.json())

        if response.status_code == 201:
            deserialized = _deserialize(_models.Deployment, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_deployment(  # pylint: disable=inconsistent-return-statements
        self, deployment_id: str, **kwargs: Any
    ) -> None:
        """Deletes the deployment specified by the given deployment_id.

        Deletes the deployment specified by the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        request = build_authoring_delete_deployment_request(
            deployment_id=deployment_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [204]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if cls:
            return cls(pipeline_response, None, {})

    @distributed_trace_async
    async def list_files(self, **kwargs: Any) -> _models.FileList:
        """Gets a list of all files owned by the Azure OpenAI resource.
        These include user uploaded content like files with purpose "fine-tune" for training or
        validation of fine-tunes models as well as files that are generated by the
        service such as "fine-tune-results" which contains various metrics for the
        corresponding fine-tune job.

        Gets a list of all files owned by the Azure OpenAI resource.
        These include user uploaded content like files with purpose "fine-tune" for training or
        validation of fine-tunes models
        as well as files that are generated by the
        service such as "fine-tune-results" which contains various metrics for the
        corresponding fine-tune job.

        :return: FileList. The FileList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FileList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FileList] = kwargs.pop("cls", None)

        request = build_authoring_list_files_request(
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FileList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def upload_file(
        self, body: _models.File, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CustomResponseFields:
        """Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        :param body: Required.
        :type body: ~azure.openai.python.models.File
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: CustomResponseFields. The CustomResponseFields is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.CustomResponseFields
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def upload_file(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CustomResponseFields:
        """Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: CustomResponseFields. The CustomResponseFields is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.CustomResponseFields
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def upload_file(
        self, body: IO, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CustomResponseFields:
        """Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        :param body: Required.
        :type body: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: CustomResponseFields. The CustomResponseFields is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.CustomResponseFields
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def upload_file(self, body: Union[_models.File, JSON, IO], **kwargs: Any) -> _models.CustomResponseFields:
        """Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        :param body: Is one of the following types: model, JSON, IO Required.
        :type body: ~azure.openai.python.models.File or JSON or IO
        :keyword content_type: Body parameter Content-Type. Known values are: application/json. Default
         value is None.
        :paramtype content_type: str
        :return: CustomResponseFields. The CustomResponseFields is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.CustomResponseFields
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.CustomResponseFields] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IO, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=AzureJSONEncoder)  # type: ignore

        request = build_authoring_upload_file_request(
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))

        deserialized = _deserialize(_models.CustomResponseFields, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_file(self, file_id: str, **kwargs: Any) -> _models.File:
        """Gets details for a single file specified by the given file_id including status,
        size, purpose, etc.

        Gets details for a single file specified by the given file_id including status,
        size, purpose, etc.

        :param file_id: The identity of this item. Required.
        :type file_id: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.File] = kwargs.pop("cls", None)

        request = build_authoring_get_file_request(
            file_id=file_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.File, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_file(self, file_id: str, **kwargs: Any) -> None:  # pylint: disable=inconsistent-return-statements
        """Deletes the file with the given file_id.
        Deletion is also allowed if a file
        was used, e.g., as training file in a fine-tune job.

        Deletes the file with the given file_id.
        Deletion is also allowed if a file
        was used, e.g., as training file in a fine-tune job.

        :param file_id: The identity of this item. Required.
        :type file_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        request = build_authoring_delete_file_request(
            file_id=file_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [204]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if cls:
            return cls(pipeline_response, None, {})

    @distributed_trace_async
    async def get_file_content(self, file_id: str, **kwargs: Any) -> _models.FileContent:
        """Gets the content of the file specified by the given file_id.
        Files can be user
        uploaded content or generated by the service like result metrics of a fine-tune
        job.

        Gets the content of the file specified by the given file_id.
        Files can be user
        uploaded content or generated by the service like result metrics of a fine-tune
        job.

        :param file_id: The identity of this item. Required.
        :type file_id: str
        :return: FileContent. The FileContent is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FileContent
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FileContent] = kwargs.pop("cls", None)

        request = build_authoring_get_file_content_request(
            file_id=file_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FileContent, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def import_file(
        self, body: _models.FileImport, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.File:
        """Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        :param body: expected schema for the body of the completion post request. Required.
        :type body: ~azure.openai.python.models.FileImport
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def import_file(self, body: JSON, *, content_type: str = "application/json", **kwargs: Any) -> _models.File:
        """Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        :param body: expected schema for the body of the completion post request. Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def import_file(self, body: IO, *, content_type: str = "application/json", **kwargs: Any) -> _models.File:
        """Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        :param body: expected schema for the body of the completion post request. Required.
        :type body: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def import_file(self, body: Union[_models.FileImport, JSON, IO], **kwargs: Any) -> _models.File:
        """Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        :param body: expected schema for the body of the completion post request. Is one of the
         following types: model, JSON, IO Required.
        :type body: ~azure.openai.python.models.FileImport or JSON or IO
        :keyword content_type: Body parameter Content-Type. Known values are: application/json. Default
         value is None.
        :paramtype content_type: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.File] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IO, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=AzureJSONEncoder)  # type: ignore

        request = build_authoring_import_file_request(
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["location"] = self._deserialize("str", response.headers.get("location"))

        deserialized = _deserialize(_models.File, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def list_fine_tunes(self, **kwargs: Any) -> _models.FineTuneList:
        """Gets a list of all fine-tune jobs owned by the Azure OpenAI resource.
        The details that are returned for each fine-tune job contain besides its
        identifier the base model, training and validation files, hyper parameters,
        time stamps, status and events.  Events are created when the job status
        changes, e.g. running or complete, and when results are uploaded.

        Gets a list of all fine-tune jobs owned by the Azure OpenAI resource.
        The details that are returned for each fine-tune job contain besides its
        identifier the base model, training and validation files, hyper parameters,
        time stamps, status and events. Events are created when the job status
        changes, e.g. running or complete, and when results are uploaded.

        :return: FineTuneList. The FineTuneList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTuneList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FineTuneList] = kwargs.pop("cls", None)

        request = build_authoring_list_fine_tunes_request(
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FineTuneList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def create_fine_tune(
        self, body: _models.FineTuneCreation, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FineTune:
        """Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        :param body: Required.
        :type body: ~azure.openai.python.models.FineTuneCreation
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_fine_tune(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FineTune:
        """Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_fine_tune(
        self, body: IO, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FineTune:
        """Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        :param body: Required.
        :type body: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def create_fine_tune(
        self, body: Union[_models.FineTuneCreation, JSON, IO], **kwargs: Any
    ) -> _models.FineTune:
        """Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        :param body: Is one of the following types: model, JSON, IO Required.
        :type body: ~azure.openai.python.models.FineTuneCreation or JSON or IO
        :keyword content_type: Body parameter Content-Type. Known values are: application/json. Default
         value is None.
        :paramtype content_type: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.FineTune] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IO, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=AzureJSONEncoder)  # type: ignore

        request = build_authoring_create_fine_tune_request(
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["location"] = self._deserialize("str", response.headers.get("location"))

        deserialized = _deserialize(_models.FineTune, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_fine_tune(self, fine_tune_id: str, **kwargs: Any) -> _models.FineTune:
        """Gets details for a single fine-tune job specified by the given
        fine_tune_id.
        The details contain the base model, training and validation
        files, hyper parameters, time stamps, status and events.
        Events are created
        when the job status changes, e.g. running or complete, and when results are
        uploaded.

        Gets details for a single fine-tune job specified by the given
        fine_tune_id.
        The details contain the base model, training and validation
        files, hyper parameters, time stamps, status and events.
        Events are created
        when the job status changes, e.g. running or complete, and when results are
        uploaded.

        :param fine_tune_id: The identity of this item. Required.
        :type fine_tune_id: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FineTune] = kwargs.pop("cls", None)

        request = build_authoring_get_fine_tune_request(
            fine_tune_id=fine_tune_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FineTune, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_fine_tune(  # pylint: disable=inconsistent-return-statements
        self, fine_tune_id: str, **kwargs: Any
    ) -> None:
        """Deletes the fine-tune job specified by the given fine_tune_id.

        Deletes the fine-tune job specified by the given fine_tune_id.

        :param fine_tune_id: The identity of this item. Required.
        :type fine_tune_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        request = build_authoring_delete_fine_tune_request(
            fine_tune_id=fine_tune_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [204]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if cls:
            return cls(pipeline_response, None, {})

    @distributed_trace_async
    async def list_fine_tune_events(self, fine_tune_id: str, *, stream: bool, **kwargs: Any) -> _models.EventList:
        """List events for the fine-tune job specified by the given fine_tune_id.
        Events are created when the job status changes, e.g. running or
        complete, and when results are uploaded.

        List events for the fine-tune job specified by the given fine_tune_id.
        Events are created when the job status changes, e.g. running or
        complete, and when results are uploaded.

        :param fine_tune_id: The identity of this item. Required.
        :type fine_tune_id: str
        :keyword stream: A flag indicating whether to stream events for the fine-tune job. If set to
         true,
         events will be sent as data-only server-sent events as they become available. The stream will
         terminate with
         a data: [DONE] message when the job is finished (succeeded, cancelled, or failed).
         If set to false, only events generated so far will be returned.. Required.
        :paramtype stream: bool
        :return: EventList. The EventList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.EventList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.EventList] = kwargs.pop("cls", None)

        request = build_authoring_list_fine_tune_events_request(
            fine_tune_id=fine_tune_id,
            stream=stream,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.EventList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def cancel_fine_tune(self, fine_tune_id: str, **kwargs: Any) -> _models.FineTune:
        """Cancels the processing of the fine-tune job specified by the given fine_tune_id.

        Cancels the processing of the fine-tune job specified by the given fine_tune_id.

        :param fine_tune_id: The identity of this item. Required.
        :type fine_tune_id: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FineTune] = kwargs.pop("cls", None)

        request = build_authoring_cancel_fine_tune_request(
            fine_tune_id=fine_tune_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FineTune, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def list_models(self, **kwargs: Any) -> _models.ModelList:
        """Gets a list of all models that are accessible by the Azure OpenAI
        resource.
        These include base models as well as all successfully completed
        fine-tuned models owned by the Azure OpenAI resource.

        Gets a list of all models that are accessible by the Azure OpenAI
        resource.
        These include base models as well as all successfully completed
        fine-tuned models owned by the Azure OpenAI resource.

        :return: ModelList. The ModelList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.ModelList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.ModelList] = kwargs.pop("cls", None)

        request = build_authoring_list_models_request(
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.ModelList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_model(self, model_id: str, **kwargs: Any) -> _models.Model:
        """Gets details for the model specified by the given model_id.

        Gets details for the model specified by the given model_id.

        :param model_id: The identity of this item. Required.
        :type model_id: str
        :return: Model. The Model is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Model
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.Model] = kwargs.pop("cls", None)

        request = build_authoring_get_model_request(
            model_id=model_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.Model, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore


class InferenceClientOperationsMixin(InferenceClientMixinABC):
    @overload
    async def embeddings(
        self, deployment_id: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Embeddings:
        """Return the embeddings for a given prompt.

        :param deployment_id: deployment id of the deployed model. Required.
        :type deployment_id: str
        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Embeddings. The Embeddings is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Embeddings
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                body = {
                    "input": "str",  # An input to embed, encoded as a string, a list of strings,
                      or a list of token"nlists. Required. Is one of the following types: string, list,
                      list, list
                    "input_type": "str",  # Optional. input type of embedding search to use.
                    "model": "str",  # Optional. ID of the model to use.
                    "user": "str"  # Optional. The ID of the end-user, for use in tracking and
                      rate-limiting.
                }
        """

    @overload
    async def embeddings(
        self,
        deployment_id: str,
        *,
        input: Union[str, List[str], List[int], List[List[int]]],
        content_type: str = "application/json",
        user: Optional[str] = None,
        input_type: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs: Any
    ) -> _models.Embeddings:
        """Return the embeddings for a given prompt.

        :param deployment_id: deployment id of the deployed model. Required.
        :type deployment_id: str
        :keyword input: An input to embed, encoded as a string, a list of strings, or a list of token
         lists. Is one of the following types: string, list, list, list Required.
        :paramtype input: str or list[str] or list[int] or list[list[int]]
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword user: The ID of the end-user, for use in tracking and rate-limiting. Default value is
         None.
        :paramtype user: str
        :keyword input_type: input type of embedding search to use. Default value is None.
        :paramtype input_type: str
        :keyword model: ID of the model to use. Default value is None.
        :paramtype model: str
        :return: Embeddings. The Embeddings is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Embeddings
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def embeddings(
        self, deployment_id: str, body: IO, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Embeddings:
        """Return the embeddings for a given prompt.

        :param deployment_id: deployment id of the deployed model. Required.
        :type deployment_id: str
        :param body: Required.
        :type body: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Embeddings. The Embeddings is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Embeddings
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def embeddings(
        self,
        deployment_id: str,
        body: Union[JSON, IO],
        *,
        input: Union[str, List[str], List[int], List[List[int]]],
        user: Optional[str] = None,
        input_type: Optional[str] = None,
        model: Optional[str] = None,
        **kwargs: Any
    ) -> _models.Embeddings:
        """Return the embeddings for a given prompt.

        :param deployment_id: deployment id of the deployed model. Required.
        :type deployment_id: str
        :param body: Is either a model type or a IO type. Required.
        :type body: JSON or IO
        :keyword input: An input to embed, encoded as a string, a list of strings, or a list of token
         lists. Is one of the following types: string, list, list, list Required.
        :paramtype input: str or list[str] or list[int] or list[list[int]]
        :keyword user: The ID of the end-user, for use in tracking and rate-limiting. Default value is
         None.
        :paramtype user: str
        :keyword input_type: input type of embedding search to use. Default value is None.
        :paramtype input_type: str
        :keyword model: ID of the model to use. Default value is None.
        :paramtype model: str
        :keyword content_type: Body parameter Content-Type. Known values are: application/json. Default
         value is None.
        :paramtype content_type: str
        :return: Embeddings. The Embeddings is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Embeddings
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Embeddings] = kwargs.pop("cls", None)

        if body is None:
            body = {"input": input, "input_type": input_type, "model": model, "user": user}
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IO, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=AzureJSONEncoder)  # type: ignore

        request = build_inference_embeddings_request(
            deployment_id=deployment_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.Embeddings, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def completions(
        self, deployment_id: str, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Completion:
        """Return the completions for a given prompt.

        :param deployment_id: deployment id of the deployed model. Required.
        :type deployment_id: str
        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Completion. The Completion is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Completion
        :raises ~azure.core.exceptions.HttpResponseError:

        Example:
            .. code-block:: python

                # JSON input template you can fill out and use as your body input.
                body = {
                    "best_of": 0,  # Optional. How many generations to create server side, and
                      display only the best. Will not"nstream intermediate progress if best_of > 1. Has
                      maximum value of 128.
                    "cache_level": 0,  # Optional. can be used to disable any server-side
                      caching, 0=no cache, 1=prompt prefix"nenabled, 2=full cache.
                    "completion_config": "str",  # Optional. Completion configuration.
                    "echo": bool,  # Optional. Echo back the prompt in addition to the
                      completion.
                    "frequency_penalty": 0.0,  # Optional. How much to penalize new tokens based
                      on whether they appear in the text so"nfar. Increases the model's likelihood to
                      talk about new topics.
                    "logit_bias": {
                        "str": 0  # Optional. Defaults to null. Modify the likelihood of
                          specified tokens appearing in the"ncompletion. Accepts a json object that
                          maps tokens (specified by their token ID"nin the GPT tokenizer) to an
                          associated bias value from -100 to 100. You can use"nthis tokenizer tool
                          (which works for both GPT-2 and GPT-3) to convert text to"ntoken IDs.
                          Mathematically, the bias is added to the logits generated by the"nmodel prior
                          to sampling. The exact effect will vary per model, but values"nbetween -1 and
                          1 should decrease or increase likelihood of selection; values"nlike -100 or
                          100 should result in a ban or exclusive selection of the relevant"ntoken. As
                          an example, you can pass {"50256" &#58; -100} to prevent the"n<|endoftext|>
                          token from being generated.
                    },
                    "logprobs": 0,  # Optional. Include the log probabilities on the ``logprobs``
                      most likely tokens, as well the"nchosen tokens. So for example, if ``logprobs``
                      is 10, the API will return a list"nof the 10 most likely tokens. If ``logprobs``
                      is 0, only the chosen tokens will"nhave logprobs returned. Minimum of 0 and
                      maximum of 100 allowed.
                    "max_tokens": 0,  # Optional. The maximum number of tokens to generate. Has
                      minimum of 0.
                    "model": "str",  # Optional. The name of the model to use.
                    "n": 0,  # Optional. How many snippets to generate for each prompt. Minimum
                      of 1 and maximum of 128"nallowed.
                    "presence_penalty": 0.0,  # Optional. How much to penalize new tokens based
                      on their existing frequency in the text"nso far. Decreases the model's likelihood
                      to repeat the same line verbatim. Has"nminimum of -2 and maximum of 2.
                    "prompt": "str",  # Optional. An optional prompt to complete from, encoded as
                      a string, a list of strings, or"na list of token lists. Defaults to
                      <|endoftext|>. The prompt to complete from."nIf you would like to provide
                      multiple prompts, use the POST variant of this"nmethod. Note that <|endoftext|>
                      is the document separator that the model sees"nduring training, so if a prompt is
                      not specified the model will generate as if"nfrom the beginning of a new
                      document. Maximum allowed size of string list is"n2048. Is one of the following
                      types: string, list, list
                    "stop": "str",  # Optional. A sequence which indicates the end of the current
                      document. Is either a string type or a list type.
                    "stream": bool,  # Optional. Whether to enable streaming for this endpoint.
                      If set, tokens will be sent as"nserver-sent events as they become available.
                    "temperature": 0.0,  # Optional. What sampling temperature to use. Higher
                      values means the model will take more"nrisks. Try 0.9 for more creative
                      applications, and 0 (argmax sampling) for ones"nwith a well-defined answer."nWe
                      generally recommend using this or ``top_p`` but"nnot both."nMinimum of 0 and
                      maximum of 2 allowed.
                    "top_p": 0.0,  # Optional. An alternative to sampling with temperature,
                      called nucleus sampling, where the"nmodel considers the results of the tokens
                      with top_p probability mass. So 0.1"nmeans only the tokens comprising the top 10%
                      probability mass are"nconsidered."nWe generally recommend using this or
                      ``temperature`` but not"nboth."nMinimum of 0 and maximum of 1 allowed.
                    "user": "str"  # Optional. The ID of the end-user, for use in tracking and
                      rate-limiting.
                }
        """

    @overload
    async def completions(
        self,
        deployment_id: str,
        *,
        content_type: str = "application/json",
        prompt: Optional[Union[str, List[str], List[List[str]]]] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        logit_bias: Optional[Dict[str, int]] = None,
        user: Optional[str] = None,
        n: Optional[int] = None,
        stream: Optional[bool] = None,
        logprobs: Optional[int] = None,
        model: Optional[str] = None,
        echo: Optional[bool] = None,
        stop: Optional[Union[str, List[str]]] = None,
        completion_config: Optional[str] = None,
        cache_level: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        best_of: Optional[int] = None,
        **kwargs: Any
    ) -> _models.Completion:
        """Return the completions for a given prompt.

        :param deployment_id: deployment id of the deployed model. Required.
        :type deployment_id: str
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :keyword prompt: An optional prompt to complete from, encoded as a string, a list of strings,
         or
         a list of token lists. Defaults to <|endoftext|>. The prompt to complete from.
         If you would like to provide multiple prompts, use the POST variant of this
         method. Note that <|endoftext|> is the document separator that the model sees
         during training, so if a prompt is not specified the model will generate as if
         from the beginning of a new document. Maximum allowed size of string list is
         2048. Is one of the following types: string, list, list Default value is None.
        :paramtype prompt: str or list[str] or list[list[str]]
        :keyword max_tokens: The maximum number of tokens to generate. Has minimum of 0. Default value
         is None.
        :paramtype max_tokens: int
        :keyword temperature: What sampling temperature to use. Higher values means the model will take
         more
         risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones
         with a well-defined answer.
         We generally recommend using this or ``top_p`` but
         not both.
         Minimum of 0 and maximum of 2 allowed. Default value is None.
        :paramtype temperature: float
        :keyword top_p: An alternative to sampling with temperature, called nucleus sampling, where the
         model considers the results of the tokens with top_p probability mass. So 0.1
         means only the tokens comprising the top 10% probability mass are
         considered.
         We generally recommend using this or ``temperature`` but not
         both.
         Minimum of 0 and maximum of 1 allowed. Default value is None.
        :paramtype top_p: float
        :keyword logit_bias: Defaults to null. Modify the likelihood of specified tokens appearing in
         the
         completion. Accepts a json object that maps tokens (specified by their token ID
         in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
         this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
         token IDs. Mathematically, the bias is added to the logits generated by the
         model prior to sampling. The exact effect will vary per model, but values
         between -1 and 1 should decrease or increase likelihood of selection; values
         like -100 or 100 should result in a ban or exclusive selection of the relevant
         token. As an example, you can pass {"50256" &#58; -100} to prevent the
         <|endoftext|> token from being generated. Default value is None.
        :paramtype logit_bias: dict[str, int]
        :keyword user: The ID of the end-user, for use in tracking and rate-limiting. Default value is
         None.
        :paramtype user: str
        :keyword n: How many snippets to generate for each prompt. Minimum of 1 and maximum of 128
         allowed. Default value is None.
        :paramtype n: int
        :keyword stream: Whether to enable streaming for this endpoint. If set, tokens will be sent as
         server-sent events as they become available. Default value is None.
        :paramtype stream: bool
        :keyword logprobs: Include the log probabilities on the ``logprobs`` most likely tokens, as
         well the
         chosen tokens. So for example, if ``logprobs`` is 10, the API will return a list
         of the 10 most likely tokens. If ``logprobs`` is 0, only the chosen tokens will
         have logprobs returned. Minimum of 0 and maximum of 100 allowed. Default value is None.
        :paramtype logprobs: int
        :keyword model: The name of the model to use. Default value is None.
        :paramtype model: str
        :keyword echo: Echo back the prompt in addition to the completion. Default value is None.
        :paramtype echo: bool
        :keyword stop: A sequence which indicates the end of the current document. Is either a string
         type or a list type. Default value is None.
        :paramtype stop: str or list[str]
        :keyword completion_config: Completion configuration. Default value is None.
        :paramtype completion_config: str
        :keyword cache_level: can be used to disable any server-side caching, 0=no cache, 1=prompt
         prefix
         enabled, 2=full cache. Default value is None.
        :paramtype cache_level: int
        :keyword presence_penalty: How much to penalize new tokens based on their existing frequency in
         the text
         so far. Decreases the model's likelihood to repeat the same line verbatim. Has
         minimum of -2 and maximum of 2. Default value is None.
        :paramtype presence_penalty: float
        :keyword frequency_penalty: How much to penalize new tokens based on whether they appear in the
         text so
         far. Increases the model's likelihood to talk about new topics. Default value is None.
        :paramtype frequency_penalty: float
        :keyword best_of: How many generations to create server side, and display only the best. Will
         not
         stream intermediate progress if best_of > 1. Has maximum value of 128. Default value is None.
        :paramtype best_of: int
        :return: Completion. The Completion is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Completion
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def completions(
        self, deployment_id: str, body: IO, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.Completion:
        """Return the completions for a given prompt.

        :param deployment_id: deployment id of the deployed model. Required.
        :type deployment_id: str
        :param body: Required.
        :type body: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: Completion. The Completion is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Completion
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def completions(
        self,
        deployment_id: str,
        body: Union[JSON, IO],
        *,
        prompt: Optional[Union[str, List[str], List[List[str]]]] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        logit_bias: Optional[Dict[str, int]] = None,
        user: Optional[str] = None,
        n: Optional[int] = None,
        stream: Optional[bool] = None,
        logprobs: Optional[int] = None,
        model: Optional[str] = None,
        echo: Optional[bool] = None,
        stop: Optional[Union[str, List[str]]] = None,
        completion_config: Optional[str] = None,
        cache_level: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        best_of: Optional[int] = None,
        **kwargs: Any
    ) -> _models.Completion:
        """Return the completions for a given prompt.

        :param deployment_id: deployment id of the deployed model. Required.
        :type deployment_id: str
        :param body: Is either a model type or a IO type. Required.
        :type body: JSON or IO
        :keyword prompt: An optional prompt to complete from, encoded as a string, a list of strings,
         or
         a list of token lists. Defaults to <|endoftext|>. The prompt to complete from.
         If you would like to provide multiple prompts, use the POST variant of this
         method. Note that <|endoftext|> is the document separator that the model sees
         during training, so if a prompt is not specified the model will generate as if
         from the beginning of a new document. Maximum allowed size of string list is
         2048. Is one of the following types: string, list, list Default value is None.
        :paramtype prompt: str or list[str] or list[list[str]]
        :keyword max_tokens: The maximum number of tokens to generate. Has minimum of 0. Default value
         is None.
        :paramtype max_tokens: int
        :keyword temperature: What sampling temperature to use. Higher values means the model will take
         more
         risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones
         with a well-defined answer.
         We generally recommend using this or ``top_p`` but
         not both.
         Minimum of 0 and maximum of 2 allowed. Default value is None.
        :paramtype temperature: float
        :keyword top_p: An alternative to sampling with temperature, called nucleus sampling, where the
         model considers the results of the tokens with top_p probability mass. So 0.1
         means only the tokens comprising the top 10% probability mass are
         considered.
         We generally recommend using this or ``temperature`` but not
         both.
         Minimum of 0 and maximum of 1 allowed. Default value is None.
        :paramtype top_p: float
        :keyword logit_bias: Defaults to null. Modify the likelihood of specified tokens appearing in
         the
         completion. Accepts a json object that maps tokens (specified by their token ID
         in the GPT tokenizer) to an associated bias value from -100 to 100. You can use
         this tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to
         token IDs. Mathematically, the bias is added to the logits generated by the
         model prior to sampling. The exact effect will vary per model, but values
         between -1 and 1 should decrease or increase likelihood of selection; values
         like -100 or 100 should result in a ban or exclusive selection of the relevant
         token. As an example, you can pass {"50256" &#58; -100} to prevent the
         <|endoftext|> token from being generated. Default value is None.
        :paramtype logit_bias: dict[str, int]
        :keyword user: The ID of the end-user, for use in tracking and rate-limiting. Default value is
         None.
        :paramtype user: str
        :keyword n: How many snippets to generate for each prompt. Minimum of 1 and maximum of 128
         allowed. Default value is None.
        :paramtype n: int
        :keyword stream: Whether to enable streaming for this endpoint. If set, tokens will be sent as
         server-sent events as they become available. Default value is None.
        :paramtype stream: bool
        :keyword logprobs: Include the log probabilities on the ``logprobs`` most likely tokens, as
         well the
         chosen tokens. So for example, if ``logprobs`` is 10, the API will return a list
         of the 10 most likely tokens. If ``logprobs`` is 0, only the chosen tokens will
         have logprobs returned. Minimum of 0 and maximum of 100 allowed. Default value is None.
        :paramtype logprobs: int
        :keyword model: The name of the model to use. Default value is None.
        :paramtype model: str
        :keyword echo: Echo back the prompt in addition to the completion. Default value is None.
        :paramtype echo: bool
        :keyword stop: A sequence which indicates the end of the current document. Is either a string
         type or a list type. Default value is None.
        :paramtype stop: str or list[str]
        :keyword completion_config: Completion configuration. Default value is None.
        :paramtype completion_config: str
        :keyword cache_level: can be used to disable any server-side caching, 0=no cache, 1=prompt
         prefix
         enabled, 2=full cache. Default value is None.
        :paramtype cache_level: int
        :keyword presence_penalty: How much to penalize new tokens based on their existing frequency in
         the text
         so far. Decreases the model's likelihood to repeat the same line verbatim. Has
         minimum of -2 and maximum of 2. Default value is None.
        :paramtype presence_penalty: float
        :keyword frequency_penalty: How much to penalize new tokens based on whether they appear in the
         text so
         far. Increases the model's likelihood to talk about new topics. Default value is None.
        :paramtype frequency_penalty: float
        :keyword best_of: How many generations to create server side, and display only the best. Will
         not
         stream intermediate progress if best_of > 1. Has maximum value of 128. Default value is None.
        :paramtype best_of: int
        :keyword content_type: Body parameter Content-Type. Known values are: application/json. Default
         value is None.
        :paramtype content_type: str
        :return: Completion. The Completion is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Completion
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Completion] = kwargs.pop("cls", None)

        if body is None:
            body = {
                "best_of": best_of,
                "cache_level": cache_level,
                "completion_config": completion_config,
                "echo": echo,
                "frequency_penalty": frequency_penalty,
                "logit_bias": logit_bias,
                "logprobs": logprobs,
                "max_tokens": max_tokens,
                "model": model,
                "n": n,
                "presence_penalty": presence_penalty,
                "prompt": prompt,
                "stop": stop,
                "stream": stream,
                "temperature": temperature,
                "top_p": top_p,
                "user": user,
            }
        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IO, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=AzureJSONEncoder)  # type: ignore

        request = build_inference_completions_request(
            deployment_id=deployment_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["apim-request-id"] = self._deserialize("str", response.headers.get("apim-request-id"))

        deserialized = _deserialize(_models.Completion, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore
