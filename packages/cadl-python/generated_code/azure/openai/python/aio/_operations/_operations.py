# pylint: disable=too-many-lines
# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for license information.
# Code generated by Microsoft (R) Python Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------
import json
import sys
from typing import Any, Callable, Dict, IO, Optional, TypeVar, Union, overload

from azure.core.exceptions import (
    ClientAuthenticationError,
    HttpResponseError,
    ResourceExistsError,
    ResourceNotFoundError,
    ResourceNotModifiedError,
    map_error,
)
from azure.core.pipeline import PipelineResponse
from azure.core.pipeline.transport import AsyncHttpResponse
from azure.core.rest import HttpRequest
from azure.core.tracing.decorator_async import distributed_trace_async
from azure.core.utils import case_insensitive_dict

from ... import models as _models
from ..._model_base import AzureJSONEncoder, _deserialize
from ..._operations._operations import (
    build_open_ai_cancel_fine_tune_request,
    build_open_ai_create_fine_tune_request,
    build_open_ai_delete_deployment_request,
    build_open_ai_delete_file_request,
    build_open_ai_delete_fine_tune_request,
    build_open_ai_get_deployment_request,
    build_open_ai_get_file_content_request,
    build_open_ai_get_file_request,
    build_open_ai_get_fine_tune_request,
    build_open_ai_get_model_request,
    build_open_ai_import_file_request,
    build_open_ai_list_deployments_request,
    build_open_ai_list_files_request,
    build_open_ai_list_fine_tune_events_request,
    build_open_ai_list_fine_tunes_request,
    build_open_ai_list_models_request,
    build_open_ai_update_deployment_request,
    build_open_ai_upload_file_request,
)
from .._vendor import OpenAIClientMixinABC

if sys.version_info >= (3, 9):
    from collections.abc import MutableMapping
else:
    from typing import MutableMapping  # type: ignore  # pylint: disable=ungrouped-imports
JSON = MutableMapping[str, Any]  # pylint: disable=unsubscriptable-object
T = TypeVar("T")
ClsType = Optional[Callable[[PipelineResponse[HttpRequest, AsyncHttpResponse], T, Dict[str, Any]], Any]]


class OpenAIClientOperationsMixin(OpenAIClientMixinABC):
    @distributed_trace_async
    async def list_deployments(self, **kwargs: Any) -> _models.DeploymentList:
        """Gets the list of deployments owned by the Azure OpenAI resource.

        Gets the list of deployments owned by the Azure OpenAI resource.

        :return: DeploymentList. The DeploymentList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.DeploymentList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.DeploymentList] = kwargs.pop("cls", None)

        request = build_open_ai_list_deployments_request(
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.DeploymentList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_deployment(self, deployment_id: str, **kwargs: Any) -> _models.Deployment:
        """Gets details for a single deployment specified by the given deployment_id.

        Gets details for a single deployment specified by the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.Deployment] = kwargs.pop("cls", None)

        request = build_open_ai_get_deployment_request(
            deployment_id=deployment_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.Deployment, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def update_deployment(
        self,
        deployment_id: str,
        resource: _models.Deployment,
        *,
        content_type: str = "application/merge-patch+json",
        **kwargs: Any
    ) -> _models.Deployment:
        """Updates the mutable details of the deployment with the given deployment_id.

        Updates the mutable details of the deployment with the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :param resource: The resource instance. Required.
        :type resource: ~azure.openai.python.models.Deployment
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/merge-patch+json".
        :paramtype content_type: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def update_deployment(
        self, deployment_id: str, resource: JSON, *, content_type: str = "application/merge-patch+json", **kwargs: Any
    ) -> _models.Deployment:
        """Updates the mutable details of the deployment with the given deployment_id.

        Updates the mutable details of the deployment with the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :param resource: The resource instance. Required.
        :type resource: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/merge-patch+json".
        :paramtype content_type: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def update_deployment(
        self, deployment_id: str, resource: IO, *, content_type: str = "application/merge-patch+json", **kwargs: Any
    ) -> _models.Deployment:
        """Updates the mutable details of the deployment with the given deployment_id.

        Updates the mutable details of the deployment with the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :param resource: The resource instance. Required.
        :type resource: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/merge-patch+json".
        :paramtype content_type: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def update_deployment(
        self, deployment_id: str, resource: Union[_models.Deployment, JSON, IO], **kwargs: Any
    ) -> _models.Deployment:
        """Updates the mutable details of the deployment with the given deployment_id.

        Updates the mutable details of the deployment with the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :param resource: The resource instance. Is one of the following types: model, JSON, IO
         Required.
        :type resource: ~azure.openai.python.models.Deployment or JSON or IO
        :keyword content_type: This request has a JSON Merge Patch body. Default value is None.
        :paramtype content_type: str
        :return: Deployment. The Deployment is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Deployment
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.Deployment] = kwargs.pop("cls", None)

        content_type = content_type or "application/merge-patch+json"
        _content = None
        if isinstance(resource, (IO, bytes)):
            _content = resource
        else:
            _content = json.dumps(resource, cls=AzureJSONEncoder)  # type: ignore

        request = build_open_ai_update_deployment_request(
            deployment_id=deployment_id,
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200, 201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if response.status_code == 200:
            deserialized = _deserialize(_models.Deployment, response.json())

        if response.status_code == 201:
            deserialized = _deserialize(_models.Deployment, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_deployment(  # pylint: disable=inconsistent-return-statements
        self, deployment_id: str, **kwargs: Any
    ) -> None:
        """Deletes the deployment specified by the given deployment_id.

        Deletes the deployment specified by the given deployment_id.

        :param deployment_id: The identifier of the deployment. Required.
        :type deployment_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        request = build_open_ai_delete_deployment_request(
            deployment_id=deployment_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [204]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if cls:
            return cls(pipeline_response, None, {})

    @distributed_trace_async
    async def list_files(self, **kwargs: Any) -> _models.FileList:
        """Gets a list of all files owned by the Azure OpenAI resource.
        These include user uploaded content like files with purpose "fine-tune" for training or
        validation of fine-tunes models as well as files that are generated by the
        service such as "fine-tune-results" which contains various metrics for the
        corresponding fine-tune job.

        Gets a list of all files owned by the Azure OpenAI resource.
        These include user uploaded content like files with purpose "fine-tune" for training or
        validation of fine-tunes models
        as well as files that are generated by the
        service such as "fine-tune-results" which contains various metrics for the
        corresponding fine-tune job.

        :return: FileList. The FileList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FileList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FileList] = kwargs.pop("cls", None)

        request = build_open_ai_list_files_request(
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FileList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def upload_file(
        self, resource: _models.File, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CustomResponseFields:
        """Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        :param resource: The resource instance. Required.
        :type resource: ~azure.openai.python.models.File
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: CustomResponseFields. The CustomResponseFields is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.CustomResponseFields
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def upload_file(
        self, resource: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CustomResponseFields:
        """Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        :param resource: The resource instance. Required.
        :type resource: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: CustomResponseFields. The CustomResponseFields is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.CustomResponseFields
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def upload_file(
        self, resource: IO, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.CustomResponseFields:
        """Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        :param resource: The resource instance. Required.
        :type resource: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: CustomResponseFields. The CustomResponseFields is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.CustomResponseFields
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def upload_file(self, resource: Union[_models.File, JSON, IO], **kwargs: Any) -> _models.CustomResponseFields:
        """Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by uploading data from a local machine. Uploaded
        files can, for example, be used for training or evaluating fine-tuned models.

        :param resource: The resource instance. Is one of the following types: model, JSON, IO
         Required.
        :type resource: ~azure.openai.python.models.File or JSON or IO
        :keyword content_type: Body parameter Content-Type. Known values are: application/json. Default
         value is None.
        :paramtype content_type: str
        :return: CustomResponseFields. The CustomResponseFields is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.CustomResponseFields
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.CustomResponseFields] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(resource, (IO, bytes)):
            _content = resource
        else:
            _content = json.dumps(resource, cls=AzureJSONEncoder)  # type: ignore

        request = build_open_ai_upload_file_request(
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["Location"] = self._deserialize("str", response.headers.get("Location"))

        deserialized = _deserialize(_models.CustomResponseFields, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_file(self, file_id: str, **kwargs: Any) -> _models.File:
        """Gets details for a single file specified by the given file_id including status,
        size, purpose, etc.

        Gets details for a single file specified by the given file_id including status,
        size, purpose, etc.

        :param file_id: The identity of this item. Required.
        :type file_id: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.File] = kwargs.pop("cls", None)

        request = build_open_ai_get_file_request(
            file_id=file_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.File, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_file(self, file_id: str, **kwargs: Any) -> None:  # pylint: disable=inconsistent-return-statements
        """Deletes the file with the given file_id.
        Deletion is also allowed if a file
        was used, e.g., as training file in a fine-tune job.

        Deletes the file with the given file_id.
        Deletion is also allowed if a file
        was used, e.g., as training file in a fine-tune job.

        :param file_id: The identity of this item. Required.
        :type file_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        request = build_open_ai_delete_file_request(
            file_id=file_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [204]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if cls:
            return cls(pipeline_response, None, {})

    @distributed_trace_async
    async def get_file_content(self, file_id: str, **kwargs: Any) -> _models.FileContent:
        """Gets the content of the file specified by the given file_id.
        Files can be user
        uploaded content or generated by the service like result metrics of a fine-tune
        job.

        Gets the content of the file specified by the given file_id.
        Files can be user
        uploaded content or generated by the service like result metrics of a fine-tune
        job.

        :param file_id: The identity of this item. Required.
        :type file_id: str
        :return: FileContent. The FileContent is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FileContent
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FileContent] = kwargs.pop("cls", None)

        request = build_open_ai_get_file_content_request(
            file_id=file_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FileContent, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def import_file(
        self, body: _models.FileImport, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.File:
        """Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        :param body: expected schema for the body of the completion post request. Required.
        :type body: ~azure.openai.python.models.FileImport
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def import_file(self, body: JSON, *, content_type: str = "application/json", **kwargs: Any) -> _models.File:
        """Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        :param body: expected schema for the body of the completion post request. Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def import_file(self, body: IO, *, content_type: str = "application/json", **kwargs: Any) -> _models.File:
        """Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        :param body: expected schema for the body of the completion post request. Required.
        :type body: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def import_file(self, body: Union[_models.FileImport, JSON, IO], **kwargs: Any) -> _models.File:
        """Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        Creates a new file entity by importing data from a provided url. Uploaded files
        can, for example, be used for training or evaluating fine-tuned models.

        :param body: expected schema for the body of the completion post request. Is one of the
         following types: model, JSON, IO Required.
        :type body: ~azure.openai.python.models.FileImport or JSON or IO
        :keyword content_type: Body parameter Content-Type. Known values are: application/json. Default
         value is None.
        :paramtype content_type: str
        :return: File. The File is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.File
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.File] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IO, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=AzureJSONEncoder)  # type: ignore

        request = build_open_ai_import_file_request(
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["location"] = self._deserialize("str", response.headers.get("location"))

        deserialized = _deserialize(_models.File, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def list_fine_tunes(self, **kwargs: Any) -> _models.FineTuneList:
        """Gets a list of all fine-tune jobs owned by the Azure OpenAI resource.
        The details that are returned for each fine-tune job contain besides its
        identifier the base model, training and validation files, hyper parameters,
        time stamps, status and events.  Events are created when the job status
        changes, e.g. running or complete, and when results are uploaded.

        Gets a list of all fine-tune jobs owned by the Azure OpenAI resource.
        The details that are returned for each fine-tune job contain besides its
        identifier the base model, training and validation files, hyper parameters,
        time stamps, status and events. Events are created when the job status
        changes, e.g. running or complete, and when results are uploaded.

        :return: FineTuneList. The FineTuneList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTuneList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FineTuneList] = kwargs.pop("cls", None)

        request = build_open_ai_list_fine_tunes_request(
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FineTuneList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @overload
    async def create_fine_tune(
        self, body: _models.FineTuneCreation, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FineTune:
        """Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        :param body: Required.
        :type body: ~azure.openai.python.models.FineTuneCreation
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_fine_tune(
        self, body: JSON, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FineTune:
        """Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        :param body: Required.
        :type body: JSON
        :keyword content_type: Body Parameter content-type. Content type parameter for JSON body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @overload
    async def create_fine_tune(
        self, body: IO, *, content_type: str = "application/json", **kwargs: Any
    ) -> _models.FineTune:
        """Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        :param body: Required.
        :type body: IO
        :keyword content_type: Body Parameter content-type. Content type parameter for binary body.
         Default value is "application/json".
        :paramtype content_type: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """

    @distributed_trace_async
    async def create_fine_tune(
        self, body: Union[_models.FineTuneCreation, JSON, IO], **kwargs: Any
    ) -> _models.FineTune:
        """Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        Creates a job that fine-tunes a specified model from a given training
        file.
        Response includes details of the enqueued job including job status and
        hyper parameters.
        The name of the fine-tuned model is added to the response
        once complete.

        :param body: Is one of the following types: model, JSON, IO Required.
        :type body: ~azure.openai.python.models.FineTuneCreation or JSON or IO
        :keyword content_type: Body parameter Content-Type. Known values are: application/json. Default
         value is None.
        :paramtype content_type: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = case_insensitive_dict(kwargs.pop("headers", {}) or {})
        _params = kwargs.pop("params", {}) or {}

        content_type: Optional[str] = kwargs.pop("content_type", _headers.pop("Content-Type", None))
        cls: ClsType[_models.FineTune] = kwargs.pop("cls", None)

        content_type = content_type or "application/json"
        _content = None
        if isinstance(body, (IO, bytes)):
            _content = body
        else:
            _content = json.dumps(body, cls=AzureJSONEncoder)  # type: ignore

        request = build_open_ai_create_fine_tune_request(
            content_type=content_type,
            api_version=self._config.api_version,
            content=_content,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [201]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        response_headers = {}
        response_headers["location"] = self._deserialize("str", response.headers.get("location"))

        deserialized = _deserialize(_models.FineTune, response.json())

        if cls:
            return cls(pipeline_response, deserialized, response_headers)  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_fine_tune(self, fine_tune_id: str, **kwargs: Any) -> _models.FineTune:
        """Gets details for a single fine-tune job specified by the given
        fine_tune_id.
        The details contain the base model, training and validation
        files, hyper parameters, time stamps, status and events.
        Events are created
        when the job status changes, e.g. running or complete, and when results are
        uploaded.

        Gets details for a single fine-tune job specified by the given
        fine_tune_id.
        The details contain the base model, training and validation
        files, hyper parameters, time stamps, status and events.
        Events are created
        when the job status changes, e.g. running or complete, and when results are
        uploaded.

        :param fine_tune_id: The identity of this item. Required.
        :type fine_tune_id: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FineTune] = kwargs.pop("cls", None)

        request = build_open_ai_get_fine_tune_request(
            fine_tune_id=fine_tune_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FineTune, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def delete_fine_tune(  # pylint: disable=inconsistent-return-statements
        self, fine_tune_id: str, **kwargs: Any
    ) -> None:
        """Deletes the fine-tune job specified by the given fine_tune_id.

        Deletes the fine-tune job specified by the given fine_tune_id.

        :param fine_tune_id: The identity of this item. Required.
        :type fine_tune_id: str
        :return: None
        :rtype: None
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[None] = kwargs.pop("cls", None)

        request = build_open_ai_delete_fine_tune_request(
            fine_tune_id=fine_tune_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [204]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        if cls:
            return cls(pipeline_response, None, {})

    @distributed_trace_async
    async def list_fine_tune_events(self, fine_tune_id: str, *, stream: bool, **kwargs: Any) -> _models.EventList:
        """List events for the fine-tune job specified by the given fine_tune_id.
        Events are created when the job status changes, e.g. running or
        complete, and when results are uploaded.

        List events for the fine-tune job specified by the given fine_tune_id.
        Events are created when the job status changes, e.g. running or
        complete, and when results are uploaded.

        :param fine_tune_id: The identity of this item. Required.
        :type fine_tune_id: str
        :keyword stream: A flag indicating whether to stream events for the fine-tune job. If set to
         true,
         events will be sent as data-only server-sent events as they become available. The stream will
         terminate with
         a data: [DONE] message when the job is finished (succeeded, cancelled, or failed).
         If set to false, only events generated so far will be returned.. Required.
        :paramtype stream: bool
        :return: EventList. The EventList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.EventList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.EventList] = kwargs.pop("cls", None)

        request = build_open_ai_list_fine_tune_events_request(
            fine_tune_id=fine_tune_id,
            stream=stream,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.EventList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def cancel_fine_tune(self, fine_tune_id: str, **kwargs: Any) -> _models.FineTune:
        """Cancels the processing of the fine-tune job specified by the given fine_tune_id.

        Cancels the processing of the fine-tune job specified by the given fine_tune_id.

        :param fine_tune_id: The identity of this item. Required.
        :type fine_tune_id: str
        :return: FineTune. The FineTune is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.FineTune
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.FineTune] = kwargs.pop("cls", None)

        request = build_open_ai_cancel_fine_tune_request(
            fine_tune_id=fine_tune_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.FineTune, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def list_models(self, **kwargs: Any) -> _models.ModelList:
        """Gets a list of all models that are accessible by the Azure OpenAI
        resource.
        These include base models as well as all successfully completed
        fine-tuned models owned by the Azure OpenAI resource.

        Gets a list of all models that are accessible by the Azure OpenAI
        resource.
        These include base models as well as all successfully completed
        fine-tuned models owned by the Azure OpenAI resource.

        :return: ModelList. The ModelList is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.ModelList
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.ModelList] = kwargs.pop("cls", None)

        request = build_open_ai_list_models_request(
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.ModelList, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore

    @distributed_trace_async
    async def get_model(self, model_id: str, **kwargs: Any) -> _models.Model:
        """Gets details for the model specified by the given model_id.

        Gets details for the model specified by the given model_id.

        :param model_id: The identity of this item. Required.
        :type model_id: str
        :return: Model. The Model is compatible with MutableMapping
        :rtype: ~azure.openai.python.models.Model
        :raises ~azure.core.exceptions.HttpResponseError:
        """
        error_map = {
            401: ClientAuthenticationError,
            404: ResourceNotFoundError,
            409: ResourceExistsError,
            304: ResourceNotModifiedError,
        }
        error_map.update(kwargs.pop("error_map", {}) or {})

        _headers = kwargs.pop("headers", {}) or {}
        _params = kwargs.pop("params", {}) or {}

        cls: ClsType[_models.Model] = kwargs.pop("cls", None)

        request = build_open_ai_get_model_request(
            model_id=model_id,
            api_version=self._config.api_version,
            headers=_headers,
            params=_params,
        )
        path_format_arguments = {
            "endpoint": self._serialize.url("self._config.endpoint", self._config.endpoint, "str", skip_quote=True),
        }
        request.url = self._client.format_url(request.url, **path_format_arguments)

        pipeline_response: PipelineResponse = await self._client._pipeline.run(  # type: ignore # pylint: disable=protected-access
            request, stream=False, **kwargs
        )

        response = pipeline_response.http_response

        if response.status_code not in [200]:
            map_error(status_code=response.status_code, response=response, error_map=error_map)
            raise HttpResponseError(response=response)

        deserialized = _deserialize(_models.Model, response.json())

        if cls:
            return cls(pipeline_response, deserialized, {})  # type: ignore

        return deserialized  # type: ignore
